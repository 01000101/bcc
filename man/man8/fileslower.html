Content-type: text/html; charset=UTF-8

<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML><HEAD><TITLE>Man page of fileslower</TITLE>
</HEAD><BODY>
<H1>fileslower</H1>
Section: Maintenance Commands (8)<BR>Updated: 2016-02-07<BR><A HREF="#index">Index</A>
<A HREF="https://iovisor.github.io/bcc">Return to Main Contents</A><HR>

<A NAME="lbAB">&nbsp;</A>
<H2>NAME</H2>

fileslower - Trace slow synchronous file reads and writes.
<A NAME="lbAC">&nbsp;</A>
<H2>SYNOPSIS</H2>

<B>fileslower [-h] [-p PID] [-a] [min_ms]</B>

<A NAME="lbAD">&nbsp;</A>
<H2>DESCRIPTION</H2>

This script uses kernel dynamic tracing of synchronous reads and writes
at the VFS interface, to identify slow file reads and writes for any file
system.
<P>
This version traces __vfs_read() and __vfs_write() and only showing
synchronous I/O (the path to new_sync_read() and new_sync_write()), and
I/O with filenames. This approach provides a view of just two file
system request types: file reads and writes. There are typically many others:
asynchronous I/O, directory operations, file handle operations, file open()s,
fflush(), etc.
<P>
WARNING: See the OVERHEAD section.
<P>
By default, a minimum millisecond threshold of 10 is used.
<P>
Since this works by tracing various kernel __vfs_*() functions using dynamic
tracing, it will need updating to match any changes to these functions. A
future version should switch to using FS tracepoints instead.
<P>
Since this uses BPF, only the root user can use this tool.
<A NAME="lbAE">&nbsp;</A>
<H2>REQUIREMENTS</H2>

CONFIG_BPF and bcc.
<A NAME="lbAF">&nbsp;</A>
<H2>OPTIONS</H2>

-p PID
Trace this PID only.
<DL COMPACT>
<DT>-a<DD>
Include non-regular file types in output (sockets, FIFOs, etc).
<DT>min_ms<DD>
Minimum I/O latency (duration) to trace, in milliseconds. Default is 10 ms.
</DL>
<A NAME="lbAG">&nbsp;</A>
<H2>EXAMPLES</H2>

<DL COMPACT>
<DT>Trace synchronous file reads and writes slower than 10 ms:<DD>
#
<B>fileslower</B>

<DT>Trace slower than 1 ms:<DD>
#
<B>fileslower 1</B>

<DT>Trace slower than 1 ms, for PID 181 only:<DD>
#
<B>fileslower -p 181 1</B>

</DL>
<A NAME="lbAH">&nbsp;</A>
<H2>FIELDS</H2>

<DL COMPACT>
<DT>TIME(s)<DD>
Time of I/O completion since the first I/O seen, in seconds.
<DT>COMM<DD>
Process name.
<DT>PID<DD>
Process ID.
<DT>D<DD>
Direction of I/O. R == read, W == write.
<DT>BYTES<DD>
Size of I/O, in bytes.
<DT>LAT(ms)<DD>
Latency (duration) of I/O, measured from when the application issued it to VFS
to when it completed. This time is inclusive of block device I/O, file system
CPU cycles, file system locks, run queue latency, etc. It's a more accurate
measure of the latency suffered by applications performing file system I/O,
than to measure this down at the block device interface.
<DT>FILENAME<DD>
A cached kernel file name (comes from dentry-&gt;d_name.name).
</DL>
<A NAME="lbAI">&nbsp;</A>
<H2>OVERHEAD</H2>

Depending on the frequency of application reads and writes, overhead can become
severe, in the worst case slowing applications by 2x. In the best case, the
overhead is negligible. Hopefully for real world workloads the overhead is
often at the lower end of the spectrum -- test before use. The reason for
high overhead is that this traces VFS reads and writes, which includes FS
cache reads and writes, and can exceed one million events per second if the
application is I/O heavy. While the instrumentation is extremely lightweight,
and uses in-kernel eBPF maps for efficient timing and filtering, multiply that
cost by one million events per second and that cost becomes a million times
worse. You can get an idea of the possible cost by just counting the
instrumented events using the bcc funccount tool, eg:
<P>

# ./funccount.py -i 1 -r '^__vfs_(read|write)$'
<P>

This also costs overhead, but is somewhat less than fileslower.
<P>

If the overhead is prohibitive for your workload, I'd recommend moving
down-stack a little from VFS into the file system functions (ext4, xfs, etc).
Look for updates to bcc for specific file system tools that do this. The
advantage of a per-file system approach is that we can trace post-cache,
greatly reducing events and overhead. The disadvantage is needing custom
tracing approaches for each different file system (whereas VFS is generic).
<A NAME="lbAJ">&nbsp;</A>
<H2>SOURCE</H2>

This is from bcc.
<DL COMPACT>
<DT><DD>
<A HREF="https://github.com/iovisor/bcc">https://github.com/iovisor/bcc</A>
</DL>
<P>

Also look in the bcc distribution for a companion _examples.txt file containing
example usage, output, and commentary for this tool.
<A NAME="lbAK">&nbsp;</A>
<H2>OS</H2>

Linux
<A NAME="lbAL">&nbsp;</A>
<H2>STABILITY</H2>

Unstable - in development.
<A NAME="lbAM">&nbsp;</A>
<H2>AUTHOR</H2>

Brendan Gregg
<A NAME="lbAN">&nbsp;</A>
<H2>SEE ALSO</H2>

<A HREF="https://iovisor.github.io/bcc?8+biosnoop">biosnoop</A>(8), <A HREF="https://iovisor.github.io/bcc?8+funccount">funccount</A>(8)
<P>

<HR>
<A NAME="index">&nbsp;</A><H2>Index</H2>
<DL>
<DT><A HREF="#lbAB">NAME</A><DD>
<DT><A HREF="#lbAC">SYNOPSIS</A><DD>
<DT><A HREF="#lbAD">DESCRIPTION</A><DD>
<DT><A HREF="#lbAE">REQUIREMENTS</A><DD>
<DT><A HREF="#lbAF">OPTIONS</A><DD>
<DT><A HREF="#lbAG">EXAMPLES</A><DD>
<DT><A HREF="#lbAH">FIELDS</A><DD>
<DT><A HREF="#lbAI">OVERHEAD</A><DD>
<DT><A HREF="#lbAJ">SOURCE</A><DD>
<DT><A HREF="#lbAK">OS</A><DD>
<DT><A HREF="#lbAL">STABILITY</A><DD>
<DT><A HREF="#lbAM">AUTHOR</A><DD>
<DT><A HREF="#lbAN">SEE ALSO</A><DD>
</DL>
<HR>
This document was created by
<A HREF="https://iovisor.github.io/bcc">man2html</A>,
using the manual pages.<BR>
Time: 17:11:49 GMT, February 25, 2020
</BODY>
</HTML>
